{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 \n",
    "    \n",
    "1. Выбрать статью из списка\n",
    "2. Реализовать подход из статьи\n",
    "3. Провести эксперимент на реальных данных\n",
    "4. Написать (этот) отчёт о задаче, методе, его особенностях, полезности, смысле результатов, перспективах\n",
    "\n",
    "\n",
    "### Задание 2\n",
    "\n",
    "Рассказать о понравившейся статье, вышедшей за последний год. Где она была бы полезна и какие возможны улучшения?\n",
    "\n",
    "### Разделы\n",
    "\n",
    "Задание 1\n",
    "- Предложенные статьи\n",
    "- О методе\n",
    "- Что я сделал\n",
    "- Улучшение и применение модели\n",
    "- Свои выводы\n",
    "\n",
    "Задание 2\n",
    "\n",
    "- Любимая статья за последний год"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предложенные статьи\n",
    "\n",
    "\\+ их краткий обзор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MelGAN](https://arxiv.org/pdf/1910.06711.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Преложили быструю параллельную модель улучшения аудиосигнала в виде спектрограммы с помощью GAN'а. Подчеркнули недостатки существующих SOTA-решений (WORLD, Char2Wav – возвращали спектрограмму, а не аудиосигнал, Wavenet, SampleRNN, WaveRNN – были авторегрессионными, а поэтому и медленными), показали чем вдохновлялись (parallel WaveNet – оказалось очень красивой идеей, Clarinet использовали Normalizing Flows). Сопоставили вышеуказанное, добавили weightnorm и завернули всё в adversarial loss с тройным дискриминатором – получилось вкусно.  \n",
    "\n",
    "В результате получили модель в 10раз быстрее конкурентов схожего качества, почти end-to-end, realtime inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Adversarial attack on Alexa](http://papers.nips.cc/paper/9362-adversarial-music-real-world-audio-adversary-against-wake-word-detection-system.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Предложили первую не-whitebox adversarial аудио атаку. Также замаскировали её под музыку, показали способность модели обманывать wake-word системы (на примере Amazon Alexa) не только виртуально, но и при звучании атаки в воздухе. Фактически они заставляли Алексу зависать тем, что не позволяли модели корректно распознать wake-word от владельца. Понизили F-меру распознавания ww Алексы с 93.4% до 11.0%. Более того, человек не заметит атаку так как она не слышна из-за музыки, под которую модель учится маскироваться.\n",
    "\n",
    "Подход является не-whitebox так как модель распознавания Алексы не была точно известна (обычно атаки проводят на точных копиях модели). Для модели очень просто собрать данные – понадобилось всего 10 различных голосов, озвучивших ww с разными интонациями. Можно показать демку на Алисе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient isolated learning representations](https://arxiv.org/pdf/1905.11786v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы предложили улучшение unsupervised подхода Contrastive Prediction Coding (CPC), чем почти не подняли качество распознавания звуков и спикеров, но значительно сократили вычислительные ресурсы на обучение модели. Опираясь на статьи о принципах работы мозга, утверждающих, что мозг максимизирует взаимную информацию разных частей одного объекта и минимизирует её для разных объектов, дополнили принципом иерархии репрезентаций: на каждом следующем слое сети за счёт жадного обучения должны создаваться более сложные признаки объекта в пределах рецептивной области.\n",
    "\n",
    "В результате авторы разбили CPC модель по слоям на модули, в каждый из которых добавив Noise Contrastive Estimation loss. Благодаря этому модель может учиться итеративно, слой за слоем, значительно экономя процессорное время (не вычисляет последующие слои, пока не обучила предыдущие) и память (не протаскивает градиенты через всю модель, градиенты текут внутри только одного обучающегося блока)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GAN-based Speech Enhancemen](https://arxiv.org/pdf/1905.04874.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Авторы утверждают, что adversarial loss плох в задачах с улучшением аудио тк оценки дискриминатора бинарны, а L1 и L2 loss’ы не избавляют от артефактов аудио и не приближают предсказания модели к идеалу. Предложили способ того как сделать сделать loss дискриминатора непрерывным для непрерывных метрик. Достигают это методом «сходного поведения» – минимизируют разницу $(D(G(z), y) - Metric(G(z), y))^2$ , где D измеряет «реалистичность» дорожки. Рассматривают подход на примере метрик PESQ и STOI. Показывают ограничения метода на тренировку с несколькими целевыми функциями.«»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [U-Net for Speech Enhancement](https://openreview.net/pdf?id=SkeRTsAcYm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Предложили использовать комплексные числа в в алгебраической форме в фильтрах свёрточной сети, взяв за основу сети архитектуру U-Net. Поменяли loss из предыдущей работы, доработали его, добавив взвешивание по мощности шума и аудиодорожки. Улучшили метрики в среднем на 3%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ShaRNN](http://papers.nips.cc/paper/9451-shallow-rnn-accurate-time-series-classification-on-resource-constrained-devices.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Предложили RNN ячейку, которая может почти параллельно обрабатывать временной ряд, не теряя большую часть информации. Зафиксировали прирост производительности, почти не уронили рекуррентность. Метод показывает, что авторы – любители деревьев, потому что они построили дерево даже из RNN ячеек, разбивая исходный временной ряд на $\\sqrt(T)$ частей и пропуская каждую часть через копию ShaRNN ячейки. Полученные $\\sqrt(T)$ hidden state'ов подаются в следующую ShaRNN, и тд, пока ряд не станет достаточно коротким для «эффективного вычисления». Дополнительно предлагают архитектуру ячейки MishaRNN.\n",
    "\n",
    "Авторы верят в (почти что) мультипликативность hidden state'ов RNN ячеек, их эксперименты показывают повышение точности на стандартных датасетх. В целом статья похожа на изобретение residual connection'ов для RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбранная статья\n",
    "\n",
    "Я выбрал статью Gradien Isolated Learning Representations (GIL), потому потому что:\n",
    "\n",
    "1. Она мне понравилась больше остальных\n",
    "2. Её реализация не была полностью понятна на момент второго прочтения, хотелось взять статью посложнее, чтобы доказать себе, что могу разобраться в сложной теме\n",
    "3. Её реализация не была простой (статьи ShaRNN и GAN SE показались мне менее техническими сложными для реализации, они умещались в шаблон class Model, for e in range(epochs), loss = criterion())\n",
    "4. Она казалась мне релевантной для указанной темы стажировки (speaker diarization)\n",
    "\n",
    "\n",
    "### Подробнее о релевантности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Во-первых, unsupervised – это всегда круто, потому что обычно задачи приходят в виде \"ну у нас нет данных, но мы хотим чтобы работало вот так\", а в случае соцсети полезных данных много, но они не размечены. Репрезентации данных значительно упрощают построение ML-моделей поверх них, это упрощает решение практических задач\n",
    "\n",
    "Во-вторых, этот подход реализует не конкретную задачу (как удаление шумов на аудио), а даёт репрезентацию голоса, что можно применить в разных проектах. Поэтому однажды обученная модель может пригодиться в разных задачах, это даёт простор для выбора интересного исследования. \n",
    "\n",
    "В-третьих, в статье сделан упор на экономию вычислительных ресурсов, что всегда важно при экспериментах на полных данных (их может быть очень много, а вот видеокарт много не бывает), а также для меня, потому что мои 2ГБ видеопамяти в ноуте давно заставляют меня урезать модельки, стараясь не уронить их качество. Увидев слова \"have reduced memory consumption by factor 2.8\", я подумал, что обойдусь без CUDA out of memory. Итеративное обучение модели тоже показалось мне отличной экономией ресурсов, ведь если вышестоящие модули не учатся в данный момент, то зачем прогонять на них данные? Вычисления на полное обучение сокращаются в 2 раза при таком подходе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# О методе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Суть и особенности\n",
    "\n",
    "Сначала о методе Contrastive Prediction Coding ([CPC](https://arxiv.org/pdf/1807.03748.pdf)). Предлагается end-to-end создание эмбеддинга для фрагмента аудио. У хорошо обученной модели эмбеддинг должен нести информацию о звуках аудио и о характере голоса спикера. Исходная аудиодорожка сворачивается до меньшей размерности с помощью $g_{enc}$, получается представление небольшой части аудио, назовём его $z_t$. Мы хотим, чтобы $z$ хранила общую для всего аудиоотрывка информацию, а не только локальную. Мы должны увеличить взаимную информацию $z_t$ со всеми t этой аудиодорожки.\n",
    "\n",
    "Для этого используем RNN ячейку – в своём hidden state она протащит контекстную информацию, совместив её с текущим патчем данных. Подогнав размерности RNN, мы получим аналогичную $z_t$ репрезентацию, но уже совмещённую с прошлой информацией.\n",
    "\n",
    "Чтобы выделять отличительные особенности аудио, будем пытаться предугадать следующие $z_{t+k}$, исходя из $z_t$, как показано на рисунке. С помощью некоторого преобразования мы попытаемся оценить $z_{t+k}$. Во время обучения модель будет стараться уменьшить расхождение между своей оценкой и действительными будущими z. \n",
    "\n",
    "Предугадывание помогает научиться лучше улавливать паттерны звука, это повышает точность на задаче классификации звука (в статье CPC показано падение точности при уменьшении дальности предугадывания). Чтобы решить вторую задачу – определение спикера – добавляется Noise Contrastive Estimation (NCE). Это выкрученный на максимум негативный сэплинг аудиофрагментов для логистической регрессии, встроенный прямо в модель. Учась отличать текущее аудио от негативных примеров, модель учится улавливать особенности самого голоса на аудио.\n",
    "\n",
    "![CPC architecture](report_pictures/cpc_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь о Grad-Isolated. Авторы распилили монолитную архитектуру CPC на 5 модулей conv-relu-bn, добавив в каждый NCE. Таким образом, каждый модуль может сам себя обучать засчёт NCE. По мере приближения модуля к сходимости, он научится строить репрезентации, исходя из данных предыдущего модуля. Такой подход обеспечивает жадность при построении репрезентаций, делая репрезентации последнего модуля высокоуровневыми и позволяя им хранить максимум информации в минимуме места.\n",
    "\n",
    "Независимость модулей наводит на мысль о том. чтобы полностью их изолировать. Для этого создан gradient-block, который обрезает градиенты между модулями. Теперь модули точно-точно учатся только сами в себе. Исходя из независимости модулей, мы можем тренировать их поэтапно, обучая только один из них, не используя градиенты в нижележащих модулях и не запуская вышестоящие. Это позволяет оптимизировать потребление ресурсов (и cpu, и ram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полезность\n",
    "\n",
    "Модули можно обучать итеративно, отсутсвтие градиентов в нижележащих модулях освободит ресурсы для больших batchsize'ов и negative sample'ов, что повысит качество модели и освободит место для обучения других моделей на видеокартах. Задача хорошо параллелится на несколько gpu.\n",
    "\n",
    "Модульность архитектуры позволяет накинуть модулей сверху, чтобы получить ещё более сжатые репрезентации или охватить более длинную часть аудиофрагмента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что я сделал"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Реализовал модель с Gradient Isolated Learning (GIL)\n",
    "2. Реализовал модель с Contrastive Prediction Coding\n",
    "3. Оптимизировал Negative Sampling для обеих моделей\n",
    "4. Отделил конфигурации для экспериментов от моделей\n",
    "5. Обучил модели и показал графики loss'а и точности\n",
    "5. Протестировал подход на задачах классификации и показал результаты моделей\n",
    "6. Не заимствовал какой-либо код, кроме торча и sklearn'а\n",
    "7. Построил T-SNE диаграмму спикеров и звуков\n",
    "8. Предложил улучшения модели из статьи\n",
    "9. Предложил варианты использование модели в продакшене"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели\n",
    "\n",
    "В соответствующих папках лежат исходники моделей и стандартные конфиги, код для тренировки лежит в корне\n",
    "\n",
    "- GIL – оптимизированная модель, которую предлагалась реализовать\n",
    "- CPC – модель CPC с Negative Constrastive Estimation\n",
    "- CPC ablation – мой ablation study по CPC, модель не имеет негативного сэмплинга, её лосс изменён. Её результаты хуже оригинальной CPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следуя настройкам из статьи, были использованы: итеративная тренировка, $k=12$ шагов предсказания RNN, Adam с lr $2e-4$, архитектура из 5 модулей, длина аудиофрагмента 20480 (что соответствует 1.28сек), но было взято 5 негативных сэмплов вместо 10, так как вычисления на видеокартах проводил не только я. По той же причине не получилось провести тренировку на 300 эпох – мой процесс умер из-за перезагрузки сервера довольно рано, а заметил я это довольно поздно :с\n",
    "\n",
    "На иллюстрации показаны лоссы на train (сверху) и test (снизу) выборке (разбиение 0.2). На первый взгляд кажется, что модель оверфитнулась, но это не так. Если посмотреть на ось Y, то видно, что на train модель быстро сошлась к уровню 5.56, а на тесте, несмотря на видимый разброс, лосс колеблется на тысячных от значения 5.56. Интересно, что с каждым модулем растёт уровень лосса, к которому сходится модель. Для первого модуля это 5.56, для второго 5.7 и 5.61, 5.88 соответственно. Я думаю, что это связано с ошибками нижележащих слоёв. Так как последующие слои используют информацию предыдудщих как данные без возможности их оптимизации, то будут ошибаться на выбросах плохих репрезентаций.\n",
    "\n",
    "![Лоссы модулей при итеративной тренировке GIL](report_pictures/gil_train_iteratively_losses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы статьи приводили графики лосса первых 3 модулей (красный – итеративная тренировка), они имеют сходную гиперболическую форму, как и в моих результатах (только у них 1000 эпох, а также другое количество neg sample'ов, поэтому графики ниже по Y). Однако у авторов наблюдалось сильное расхождение между train и validation loss'ом на первых же модулях, в то время как в моём эксперименте расхождение было в последних двух (может быть вызваны недостаточным временем тренировки последних модулей. Я тренировал каждый модуль ~ 2 эпохи, это оказалось лучшим сеттингом)\n",
    "\n",
    "![GIL loss авторов](report_pictures/gil_train_paper_losses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике тренировка модели занимает меньше времени, чем тренировка CPC при одинаковых размерах батча, количестве негативных сэмплов и шагов предсказания. Сокращение памяти достигается блокированием градиентов между модулями. Уменьшение процессорного времени достигается итеративной тренировкой модулей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация спикеров\n",
    "\n",
    "Классификация обучалась на dev-clean части датасета librispeech. В датасете 40 спикеров, было взято деление выборки на train и test 0.15. На приведённых ниже графиках точности видно, что модель быстро набирает хорошее качество классификации несмотря то, что моделью классификации является один линейный слой. Результаты статьи по этой задаче состаляют 99.5%, мои – 93% и 70% на train и test соответственно. Модель сильно оверфитится из-за небольшого размера датасета. На большем датасете результаты были бы стабильнее.\n",
    "\n",
    "![speaker accuracy](report_pictures/speaker_accuracy.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы показать, что модель хорошо разделяет спикеров, я взял её предсказания с последнего слоя (размерности количества спикеров – 40) и применил T-SNE. Полученные результаты отобразил на диаграмме рассеяния. На графике видно, что модель хорошо разбивает аудиозаписи разных спикеров на кластеры, несмотря на то, что получала на вход всего 1.28 секунды записи (авторы статьи делали предсказание по всей аудиодорожке).\n",
    "\n",
    "На первой диаграмме не хватило разных цветов для раскраски принадлежности разным спикерам, поэтому цвета повторяются. На второй показано меньшее количество спикеров, чтобы кластеры не были так близко прижаты друг к другу и было видно качество кластеризации.\n",
    "\n",
    "![speaker scatter large](report_pictures/speakers_40_scatter.png) | ![speaker scatter large](report_pictures/speakers_10_scatter.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация звуков\n",
    "\n",
    "Для классификации звуков датасет librispeech был выровнен с помощью kaldi toolkit, для каждой аудиозаписи были получены данные о содержащихся звуках и интервалах, на которых они произносятся. Для предсказания звуков снова брались отрывки по 1.28сек, модель предсказывала звук для каждых 10мс на этой аудио. Точность модели ниже чем на задаче классификации спикеров, но и у авторов статьи точность тоже была ниже – 62.5%. В моём эксперименте на dev-clean train и test точности получились 65% и 51%. Низкие результаты связаны с тем, что модели нужно дольше тренироваться на большей версии датасета.\n",
    "\n",
    "![phones classification accuracy](report_pictures/phones_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для звуков так же была построена диаграмма рассеяния. На ней видно, что модель не так хорошо справляется с классификацией: при визуализации 14 звуков (из ~70) модель не очень плотно группирует их в один кластер. При визуализации большего количества звуков сложно разобрать определённые кластеры из-за наложения разных точек друг на друга. \n",
    "\n",
    "Однако есть и очень чётко оформленные кластеры, такие как silence (мета-)звук, появляющийся в начале и конце аудиодорожки, с его распознаванием модель очень хорошо справляется. В целом, модель способна выделять звуки разных спикеров в кластеры, но ей нужно намного больше времени на обучение, а данные довольно разнообразны и даже противоречивы для некоторых пар спикеров.\n",
    "\n",
    "![phones classification scatter](report_pictures/phones_14_scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшение и применение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование модели в продакшене\n",
    "\n",
    "Модель можно использовать для выделения спикеров на аудио и (аудио взятом из) видео, что может перенести фичу упоминания людей на фотках на видео, голосовые сообщения и подкасты. Более того, можно не только указать, что человек есть на записи, но и указать интервал, на котором он говорит. Это может дать возможность поиска аудиоконтента по автору\n",
    "\n",
    "Можно использовать для распознавания голоса владельца внутри приложения (какие-либо проекты с подтверждением транзакций биометрией посредством голоса / голосовым ассистентом / управлением без рук)\n",
    "\n",
    "Модель не является самостоятельной - поверх неё нужно обучать классификаторы, что делает её полезной для разных задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Возможные улучшения и исследования\n",
    "\n",
    "Теоретические\n",
    "- Попробовать использовать репрезентации модели в распознавании речи (на звуках справлялась не так уж плохо)\n",
    "- Пробовать multitask learning на phone- и speaker-classification, мб ускорит/улучшит сходимость\n",
    "- Попробовать attention в RNN (правда авторы сами указали такую возможность улучшения, не очень интересно)\n",
    "- Попробовать перевернуть модель и засунуть в GAN, ведь все так делают, чтобы выпускать новые статьи, правда??\n",
    "\n",
    "\n",
    "Технические\n",
    "- Оптимизация негативного сэмплинга (оказалось, в реализации автора статьи для аккуратного сэмплинга используется медленный выбор noise примеров, реализовал)\n",
    "- Изменение функции оценки $f_k()$ предсказываемых $z_{t+k}$ (сейчас это линейные слои, проовал 1d свёртки, результаты чуть хуже, можно попробовать навесить активаций или добавить слоёв с bottleneck'ом - должно улучшить выделение значимых признаков)\n",
    "- Сделать повторные тренировки модулей (применить Scheduler'ы к разморозке/заморозке модулей, это может ускорить сходимость при итеративной и одновременной тренировке)\n",
    "- Дистиллировать модель, потому что чекпоинт на 60МБ – это перебор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свои выводы\n",
    "\n",
    "Мне понравилось реализовывать статью, понравилась работа с аудио. Эта модальность освещена меньше остальных (например на discuss pytorch записей по ней меньше всего), а применений ML решений в больших проектах мало. Это очень информационно ёмкая модальность, и статья показывает метод сильного сжатия её данных, что делает работу с аудио удобным и привычным для ml задач (например классификации)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что не успел сделать\n",
    "\n",
    "1. Хотелось облегчить запуск экспериментов: сейчас для этого нужно менять создавать новый / изменять конфиг, сохранять отдельно папку с логами эксперимента, переносить это всё с неудобного кластера (если тренировка была там) на локальную машину – это тратит много времени. Также было бы круто запускать последовательность разных экспериментов. Мне кажется, что для этого уже есть хорошее решение, но я о нём не знаю.\n",
    "\n",
    "2. Переписать обучение на ignitе, потому что много кода просто скопировано и вставлено, это можно значительно сократить. Это улучшило бы читаемость и сохранило бы время\n",
    "\n",
    "3. Сделать multitask learning на обе задачи. После реализации моделей CPC и GIL и множества экспериментов с ними осталось желание сделать что-то новое, но не осталось времени на это. Хотелось бы и применить модели к реальным данным, увидеть решение какой-то задачки с помощью модели. \n",
    "\n",
    "4. Задать вопросы по непонятным моментам из статьи. Понадобилось много прочтений, чтобы понять как реализовать NCE подход, но остались неоднозначные моменты, в которых я не полностью уверен.\n",
    "\n",
    "5. Распараллелить вычисления на несколько видеокарт. Сравнивая свою реализацию с реализацией автора статьи, я увидел, что он использует распределённое обучение. Мне нравится такая реализация, я видел релизы библиотек вроде Horovod, и давно хотел попробовать, но не успел. \n",
    "\n",
    "6. Посмотреть на выделяемые моделью фичи. Когда свёрточные сетки стали популярны, их фильтры стали визуализировать, чтобы лучше понять их работу. Интуитивно понятно, что многослойная нейросеть для работы с данными не табличной модальности будет создавать иерархические признаки. Для CV это фильтры Габора и более сложные паттерны на верхних уровнях. Мне интересно посмотреть на фильтры в аудиомоделях на разных уровнях и попробовать их как-то визуализировать (torch captum?). Я не видел таких работ, да и в целом звук – не популярная модальность в машинном обучении (если не учитывать связанные с ганами исследования. С ганами везде и всего полно), поэтому исследовать такие вещи будет интересно и полезно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья за последний год, которая мне нравится - [AutoVC](https://arxiv.org/pdf/1905.05879.pdf) (6 Jun 2019) для стилизации речи, архитектура показана ниже.\n",
    "\n",
    "В ней предлагается модель для переноса стиля речи для аудио. Два энкодера сети (контента $E_c$ и стиля $E_s$) создают из спектрограммы репрезентации дорожек двух спикеров соответственно. Далее оба представления конкатенируются и пропускаются через bottleneck сети. Полученный тензор разворачивают декодером, получая новую спектрограмму. Её переводят в аудиодорожку с помощью вокодера (используется waveNet), получая голос, говорящий информацию первого носителя со стилем второго.\n",
    "\n",
    "Изначально отдельно $E_s$ учится сворачивать речь в эмбеддинги стиля. Далее обученный $E_s$ встраивают в сеть и замораживают. В сеть подают одну и ту же дорожку в $E_s$ и $E_c$, чтобы обучить сеть восстанавливать переданное аудио, оптимизируется reconstruction loss. Когда сеть обучилась, $E_s$ размораживают и сеть может осуществлять Voice Conversion.  \n",
    "\n",
    "![neural audio style transfer](report_pictures/autovc_style_transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему нравится\n",
    "\n",
    "Статья понравилась тем, что отдельно обучает энкодер стиля $E_s$, идея и loss функция для $E_s$ показались мне красивыми. Ещё мне нравилась идея подмены голоса, а демо-результаты этой статьи звучали очень правдоподобно. \n",
    "\n",
    "Через месяц я участвовал в хакатоне, где я и мои друзья выбрали трек Voise Conversion, где должны были решить задачу уменьшения звучания акцента на аудио. Мы посмотрели существующие решения и поняли, что autoVC является компромиссом по скорости и качеству. В открытом доступе не было полной реализации модели, поэтому мы её дописали и постарались обучить в условиях ограниченного хакатоном времени. Выход модели имел сильный металлический отзвук и множество артефактов, но для своего времени тренировки результат был +- хороший.\n",
    "\n",
    "Мы выиграли хакатон с нашей идеей и мне запомнились эмоции друзей, которые вместе держали чек победителей трека. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применения \n",
    "\n",
    "Результаты этой статьи могут быть полезны:\n",
    "- в создании фейк-новостей\n",
    "- в call-центрах, где стараются подгонять речь под единый стиль звучания\n",
    "- в медиа-индустрии для сокращения расходов и труда на озвучку разными людьми\n",
    "- в борьбе с фейк-новостями, модель можно испольковать как генератор для гана\n",
    "\n",
    "\n",
    "### Улучшения\n",
    "\n",
    "- Изменение вокодера значительно ускорит исполнение (мы изменили на SqueezeWave, стало намного быстрее, хотя сейчас мне кажется, что parallel WaveNet был бы лучше)\n",
    "- Изменение bottleneck'a избавит от траты ресурсов впустую. Лучше нашуметь или мутировать исходный сэмпл, чем (как делают авторы) выкинуть из него 31/32 информации и размазать 1/32ую по опустевшим местам, надеясь на дальнейшие мутации декодером. Их декодер огроменный, и большая часть его параметров используется чтобы восстанавливать 31/32 информации из 1/32 от неё. Подходы, нацеленные на мутацию исходного сэмпла весят меньше\n",
    "\n",
    "\n",
    "### Мои эксперименты\n",
    "\n",
    "Как упомянуто выше, мы использовали статью на хакатоне, изменяли вокодер и параллелили востановление сэмпла в декодере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
