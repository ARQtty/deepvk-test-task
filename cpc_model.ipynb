{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from hparams import Hparam\n",
    "from data.dataset import SpeechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Hparam('./cpc_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CPCModel, self).__init__()\n",
    "        strides = [5, 4, 2, 2, 2]\n",
    "        kernels = [10,8, 4, 4, 4]\n",
    "        padding = [2, 2, 2, 2, 1]\n",
    "        self.predict_steps = config.model.predict_steps\n",
    "        channels = config.model.conv_channels\n",
    "        \n",
    "        \n",
    "        self.convolutions = []\n",
    "        for i in range(5):\n",
    "            dim = config.model.conv_channels\n",
    "            if i == 0:\n",
    "                dim = 1\n",
    "            self.convolutions.append(nn.Conv1d(dim, \n",
    "                                               channels, \n",
    "                                               kernels[i], \n",
    "                                               strides[i], \n",
    "                                               padding[i]))\n",
    "            self.convolutions.append(nn.ReLU())\n",
    "            self.convolutions.append(nn.BatchNorm1d(channels))\n",
    "        self.convolutions = nn.Sequential(*self.convolutions)    \n",
    "        \n",
    "        self.autoregressor = nn.GRU(channels, \n",
    "                                    config.model.context_size, \n",
    "                                    batch_first=True)\n",
    "        \n",
    "        self.coupling_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(\n",
    "                    channels, channels, kernel_size=1)\n",
    "            )\n",
    "            for steps in range(self.predict_steps)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        z = x.view(batch_size, -1, 512)\n",
    "        ctx, state = self.autoregressor(z)\n",
    "        z = z.permute(0, 2, 1)\n",
    "\n",
    "        \n",
    "        # https://github.com/ex4sperans/freesound-classification/blob/master/networks/cpc.py\n",
    "        losses = []\n",
    "        for i in range(len(self.coupling_transforms)):\n",
    "            estimated = self.coupling_transforms[i](z)\n",
    "            #print('est shape', estimated.size())\n",
    "            \n",
    "            logits = torch.bmm(z.permute(0, 2, 1), estimated) # b x f x f\n",
    "        \n",
    "            labels = torch.eye(logits.size(2) - i).cuda()\n",
    "            labels = F.pad(labels, (0, i, i, 0))\n",
    "            labels = labels.unsqueeze(0).expand_as(logits)\n",
    "            \n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SpeechDataset(config.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPCModel(config).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "                                    Kernel Shape    Output Shape     Params  \\\n",
      "Layer                                                                         \n",
      "0_convolutions.Conv1d_0             [1, 512, 10]  [1, 512, 4095]     5.632k   \n",
      "1_convolutions.ReLU_1                          -  [1, 512, 4095]          -   \n",
      "2_convolutions.BatchNorm1d_2               [512]  [1, 512, 4095]     1.024k   \n",
      "3_convolutions.Conv1d_3            [512, 512, 8]  [1, 512, 1023]  2.097664M   \n",
      "4_convolutions.ReLU_4                          -  [1, 512, 1023]          -   \n",
      "5_convolutions.BatchNorm1d_5               [512]  [1, 512, 1023]     1.024k   \n",
      "6_convolutions.Conv1d_6            [512, 512, 4]   [1, 512, 512]  1.049088M   \n",
      "7_convolutions.ReLU_7                          -   [1, 512, 512]          -   \n",
      "8_convolutions.BatchNorm1d_8               [512]   [1, 512, 512]     1.024k   \n",
      "9_convolutions.Conv1d_9            [512, 512, 4]   [1, 512, 257]  1.049088M   \n",
      "10_convolutions.ReLU_10                        -   [1, 512, 257]          -   \n",
      "11_convolutions.BatchNorm1d_11             [512]   [1, 512, 257]     1.024k   \n",
      "12_convolutions.Conv1d_12          [512, 512, 4]   [1, 512, 128]  1.049088M   \n",
      "13_convolutions.ReLU_13                        -   [1, 512, 128]          -   \n",
      "14_convolutions.BatchNorm1d_14             [512]   [1, 512, 128]     1.024k   \n",
      "15_autoregressor                               -   [1, 128, 256]    591.36k   \n",
      "16_coupling_transforms.0.Conv1d_0  [512, 512, 1]   [1, 512, 128]   262.656k   \n",
      "17_coupling_transforms.1.Conv1d_0  [512, 512, 1]   [1, 512, 128]   262.656k   \n",
      "18_coupling_transforms.2.Conv1d_0  [512, 512, 1]   [1, 512, 128]   262.656k   \n",
      "19_coupling_transforms.3.Conv1d_0  [512, 512, 1]   [1, 512, 128]   262.656k   \n",
      "20_coupling_transforms.4.Conv1d_0  [512, 512, 1]   [1, 512, 128]   262.656k   \n",
      "\n",
      "                                      Mult-Adds  \n",
      "Layer                                            \n",
      "0_convolutions.Conv1d_0                20.9664M  \n",
      "1_convolutions.ReLU_1                         -  \n",
      "2_convolutions.BatchNorm1d_2              512.0  \n",
      "3_convolutions.Conv1d_3            2.145386496G  \n",
      "4_convolutions.ReLU_4                         -  \n",
      "5_convolutions.BatchNorm1d_5              512.0  \n",
      "6_convolutions.Conv1d_6             536.870912M  \n",
      "7_convolutions.ReLU_7                         -  \n",
      "8_convolutions.BatchNorm1d_8              512.0  \n",
      "9_convolutions.Conv1d_9             269.484032M  \n",
      "10_convolutions.ReLU_10                       -  \n",
      "11_convolutions.BatchNorm1d_11            512.0  \n",
      "12_convolutions.Conv1d_12           134.217728M  \n",
      "13_convolutions.ReLU_13                       -  \n",
      "14_convolutions.BatchNorm1d_14            512.0  \n",
      "15_autoregressor                       589.824k  \n",
      "16_coupling_transforms.0.Conv1d_0    33.554432M  \n",
      "17_coupling_transforms.1.Conv1d_0    33.554432M  \n",
      "18_coupling_transforms.2.Conv1d_0    33.554432M  \n",
      "19_coupling_transforms.3.Conv1d_0    33.554432M  \n",
      "20_coupling_transforms.4.Conv1d_0    33.554432M  \n",
      "-----------------------------------------------------------------------------------------\n",
      "                            Totals\n",
      "Total params              7.16032M\n",
      "Trainable params          7.16032M\n",
      "Non-trainable params           0.0\n",
      "Mult-Adds             3.275290112G\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "t = ds[1][1].unsqueeze(0).unsqueeze(0)\n",
    "f = summary(model, t.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=config.train.lr)\n",
    "writer = SummaryWriter()\n",
    "dl = torch.utils.data.DataLoader(ds, config.train.batch_size, shuffle=True)\n",
    "abs_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb1d4177f588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train/loss_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mabs_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(1, config.train.epochs):\n",
    "    print('Epoch %2d started' % e)\n",
    "    \n",
    "    for i, batch in enumerate(dl):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        speakers, utters = batch\n",
    "        losses = model(utters.unsqueeze(1).cuda())\n",
    "        \n",
    "        for j, loss in enumerate(losses):\n",
    "            writer.add_scalar('Train/loss_%d' % j, loss.item(), abs_step)\n",
    "            loss.backward(retain_graph=True)\n",
    "        abs_step+=1\n",
    "        \n",
    "        opt.step()\n",
    "    \n",
    "    if e % config.train.save_every == 0:\n",
    "        torch.save(model.state_dict(), config.train.save_name + '_%d_epoch.pt' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lds = torchaudio.datasets.LIBRISPEECH('/data/deepvk/librispeech/train-clean-100/', url='train-clean-100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
